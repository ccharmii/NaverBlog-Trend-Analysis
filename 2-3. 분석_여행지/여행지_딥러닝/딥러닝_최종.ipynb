{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cutad\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cutad\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = pd.read_csv('../여행지_전처리데이터/20여행지_전처리.csv', encoding='utf-8')\n",
    "df_2021 = pd.read_csv('../여행지_전처리데이터/21여행지_전처리.csv', encoding='utf-8')\n",
    "df_2022 = pd.read_csv('../여행지_전처리데이터/22여행지_전처리.csv', encoding='utf-8')\n",
    "df_2023 = pd.read_csv('../여행지_전처리데이터/23여행지_전처리.csv', encoding='utf-8')\n",
    "df_2024 = pd.read_csv('../여행지_전처리데이터/24여행지_전처리.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 계절별로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 키워드 포함 행 필터링 함수\n",
    "def filter_inclusive(df, keyword):\n",
    "    return df[df['preprocessed_body'].str.contains(keyword, na=False)]\n",
    "\n",
    "# 특정 키워드 포함하지 않는 행 제거 함수\n",
    "def filter_exclusive(df, exclude_keywords):\n",
    "    return df[~df['preprocessed_body'].str.contains('|'.join(exclude_keywords), na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2020년**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 봄 데이터프레임 생성\n",
    "spring_2020 = filter_inclusive(df_2020, '봄')\n",
    "spring_2020 = filter_exclusive(spring_2020, ['여름', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '봄' = 0 이라고 명시\n",
    "spring_2020['계절'] = 0\n",
    "\n",
    "# 여름 데이터프레임 생성\n",
    "summer_2020 = filter_inclusive(df_2020, '여름')\n",
    "summer_2020 = filter_exclusive(summer_2020, ['봄', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '여름' = 1 이라고 명시\n",
    "summer_2020['계절'] = 1\n",
    "\n",
    "# 가을 데이터프레임 생성\n",
    "autumn_2020 = filter_inclusive(df_2020, '가을')\n",
    "autumn_2020 = filter_exclusive(autumn_2020, ['봄', '여름', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '가을' = 2 이라고 명시\n",
    "autumn_2020['계절'] = 2\n",
    "\n",
    "# 겨울 데이터프레임 생성\n",
    "winter_2020 = filter_inclusive(df_2020, '겨울')\n",
    "winter_2020 = filter_exclusive(winter_2020, ['봄', '여름', '가을'])\n",
    "# 계절이라는 컬럼을 만들어서 '겨울' = 3 이라고 명시\n",
    "winter_2020['계절'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2021년**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 봄 데이터프레임 생성\n",
    "spring_2021 = filter_inclusive(df_2021, '봄')\n",
    "spring_2021 = filter_exclusive(spring_2021, ['여름', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '봄' = 0 이라고 명시\n",
    "spring_2021['계절'] = 0\n",
    "\n",
    "# 여름 데이터프레임 생성\n",
    "summer_2021 = filter_inclusive(df_2021, '여름')\n",
    "summer_2021 = filter_exclusive(summer_2021, ['봄', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '여름' = 1 이라고 명시\n",
    "summer_2021['계절'] = 1\n",
    "\n",
    "# 가을 데이터프레임 생성\n",
    "autumn_2021 = filter_inclusive(df_2021, '가을')\n",
    "autumn_2021 = filter_exclusive(autumn_2021, ['봄', '여름', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '가을' = 2 이라고 명시\n",
    "autumn_2021['계절'] = 2\n",
    "\n",
    "# 겨울 데이터프레임 생성\n",
    "winter_2021 = filter_inclusive(df_2021, '겨울')\n",
    "winter_2021 = filter_exclusive(winter_2021, ['봄', '여름', '가을'])\n",
    "# 계절이라는 컬럼을 만들어서 '겨울' = 3 이라고 명시\n",
    "winter_2021['계절'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2022년**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 봄 데이터프레임 생성\n",
    "spring_2022 = filter_inclusive(df_2022, '봄')\n",
    "spring_2022 = filter_exclusive(spring_2022, ['여름', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '봄' = 0 이라고 명시\n",
    "spring_2022['계절'] = 0\n",
    "\n",
    "# 여름 데이터프레임 생성\n",
    "summer_2022 = filter_inclusive(df_2022, '여름')\n",
    "summer_2022 = filter_exclusive(summer_2022, ['봄', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '여름' = 1 이라고 명시\n",
    "summer_2022['계절'] = 1\n",
    "\n",
    "# 가을 데이터프레임 생성\n",
    "autumn_2022 = filter_inclusive(df_2022, '가을')\n",
    "autumn_2022 = filter_exclusive(autumn_2022, ['봄', '여름', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '가을' = 2 이라고 명시\n",
    "autumn_2022['계절'] = 2\n",
    "\n",
    "# 겨울 데이터프레임 생성\n",
    "winter_2022 = filter_inclusive(df_2022, '겨울')\n",
    "winter_2022 = filter_exclusive(winter_2022, ['봄', '여름', '가을'])\n",
    "# 계절이라는 컬럼을 만들어서 '겨울' = 3 이라고 명시\n",
    "winter_2022['계절'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2023년**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 봄 데이터프레임 생성\n",
    "spring_2023 = filter_inclusive(df_2023, '봄')\n",
    "spring_2023 = filter_exclusive(spring_2023, ['여름', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '봄' = 0 이라고 명시\n",
    "spring_2023['계절'] = 0\n",
    "\n",
    "# 여름 데이터프레임 생성\n",
    "summer_2023 = filter_inclusive(df_2023, '여름')\n",
    "summer_2023 = filter_exclusive(summer_2023, ['봄', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '여름' = 1 이라고 명시\n",
    "summer_2023['계절'] = 1\n",
    "\n",
    "# 가을 데이터프레임 생성\n",
    "autumn_2023 = filter_inclusive(df_2023, '가을')\n",
    "autumn_2023 = filter_exclusive(autumn_2023, ['봄', '여름', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '가을' = 2 이라고 명시\n",
    "autumn_2023['계절'] = 2\n",
    "\n",
    "# 겨울 데이터프레임 생성\n",
    "winter_2023 = filter_inclusive(df_2023, '겨울')\n",
    "winter_2023 = filter_exclusive(winter_2023, ['봄', '여름', '가을'])\n",
    "# 계절이라는 컬럼을 만들어서 '겨울' = 3 이라고 명시\n",
    "winter_2023['계절'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2024년**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 봄 데이터프레임 생성\n",
    "spring_2024 = filter_inclusive(df_2024, '봄')\n",
    "spring_2024 = filter_exclusive(spring_2024, ['여름', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '봄' = 0 이라고 명시\n",
    "spring_2024['계절'] = 0\n",
    "\n",
    "# 여름 데이터프레임 생성\n",
    "summer_2024 = filter_inclusive(df_2024, '여름')\n",
    "summer_2024 = filter_exclusive(summer_2024, ['봄', '가을', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '여름' = 1 이라고 명시\n",
    "summer_2024['계절'] = 1\n",
    "\n",
    "# 가을 데이터프레임 생성\n",
    "autumn_2024 = filter_inclusive(df_2024, '가을')\n",
    "autumn_2024 = filter_exclusive(autumn_2024, ['봄', '여름', '겨울'])\n",
    "# 계절이라는 컬럼을 만들어서 '가을' = 2 이라고 명시\n",
    "autumn_2024['계절'] = 2\n",
    "\n",
    "# 겨울 데이터프레임 생성\n",
    "winter_2024 = filter_inclusive(df_2024, '겨울')\n",
    "winter_2024 = filter_exclusive(winter_2024, ['봄', '여름', '가을'])\n",
    "# 계절이라는 컬럼을 만들어서 '겨울' = 3 이라고 명시\n",
    "winter_2024['계절'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터프레임 합치기\n",
    "### 연도별 차이보다 계절별 차이가 더 뚜렷한 '여행지' 키워드에 대해 딥러닝으로 계절을 맞추는 태스크 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel = pd.concat([spring_2020, summer_2020, autumn_2020, winter_2020, \n",
    "                       spring_2021, summer_2021, autumn_2021, winter_2021, \n",
    "                       spring_2022, summer_2022, autumn_2022, winter_2022, \n",
    "                       spring_2023, summer_2023, autumn_2023, winter_2023, \n",
    "                       spring_2024, summer_2024, autumn_2024, winter_2024\n",
    "                       ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1921, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postdate</th>\n",
       "      <th>body</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>preprocessed_body</th>\n",
       "      <th>preprocessed_description</th>\n",
       "      <th>preprocessed_title</th>\n",
       "      <th>계절</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200108</td>\n",
       "      <td>안녕하세요 인생에 한번뿐인 여행이라고 불리기도 하는 신혼여행 그래서인지 허니문 여행...</td>\n",
       "      <td>그래서인지 허니문 b여행지b를 고를 때는 훨씬 더 신중하게 고르는데요 오늘은 b20...</td>\n",
       "      <td>신혼 여행지 추천 2020년에 제일 핫할 신혼 여행지 5곳</td>\n",
       "      <td>번 뿐 라 신혼 허니문 신중 것 은 핫 신혼 곳 것 발리 발리 신혼여행 동남아시아 ...</td>\n",
       "      <td>허니문 신중 것 은 핫 신혼 곳 것 발리 발리 신혼여행 동남아시아</td>\n",
       "      <td>신혼 신혼 곳</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200107</td>\n",
       "      <td>당신을 위한 특별한 뉴스 티티엘입니다 종합여행기업 보물섬투어가 2020년 경자년을 ...</td>\n",
       "      <td>특별한 경험 과거로 떠나는 미지의 여행 b2020년b에는 그동안 경험해 보지 못한 ...</td>\n",
       "      <td>카드뉴스 2020년 경자년 핫플레이스 여행지</td>\n",
       "      <td>뉴스 티티엘 종합 기업 보물섬 경 보물 해외 제시 눈길 과거 지 그동안 수요 예측 ...</td>\n",
       "      <td>과거 지 그동안 수요 예측 보물섬 이색 문명 박물관</td>\n",
       "      <td>카드 뉴스 경 자녀 핫 플레이스</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20240416</td>\n",
       "      <td>안녕하세요 의성군 블로그기자단 박진하입니다 오늘은 봄 향기가 가득한 단촌역 카페 이...</td>\n",
       "      <td>2001년 보통역으로 재승격되었다가 b2020년b 9월 23일 중앙선 복선전철화 공...</td>\n",
       "      <td>옛 추억의 소품들이 가득한 과거 여행지 추천 의성군 단촌역 카페 나들이</td>\n",
       "      <td>의성군 블로그 기자단 박진 은 봄 향기 단촌역 카페 단촌역 보통역 개통 되 오랫동안...</td>\n",
       "      <td>보통역 승격 중앙선 복선 전철 공사 무릉 촌 단초 고추 시장 시 단촌역 보 시 기 ...</td>\n",
       "      <td>추억 소품 과거 의성군 단촌역 카페 나들이</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200217</td>\n",
       "      <td>2020년 2월 3주차 여행지추천 삼성여행사 어플 다운로드 httpsgooglicA...</td>\n",
       "      <td>b2020년b 2월 3주차 b여행지bb추천b 삼성여행사 어플 다운로드 httpsgo...</td>\n",
       "      <td>삼성여행사 2020년 2월 3주차 여행지추천</td>\n",
       "      <td>주 차 삼성 여행사 어플 다운로드 삼성 여행사 밴드 가입 하기 삼성 여행사 카카오톡...</td>\n",
       "      <td>주 차 삼성 여행사 어플 다운로드 삼성 여행사 밴드 가입 하기 삼성 여행사 카카오톡 문</td>\n",
       "      <td>삼성 여행사 주 차</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20240418</td>\n",
       "      <td>안녕하세요 짱가입니다 어제 산청꽃잔디축제를 살펴봤는데요 오늘은 생초조각공원과 더불어...</td>\n",
       "      <td>꽃잔디축제장이었다면 b2020년b 이후 소셜네트워트를 통해 아마 가장 유명한 꽃잔디...</td>\n",
       "      <td>경남 산청대명사꽃잔디 만개 남도 4월여행지</td>\n",
       "      <td>산청 꽃잔디 축제 은 생초 조각 공원 꽃잔디 명소 산청 꽃잔디 시 터 데 산청 대명...</td>\n",
       "      <td>꽃잔디 축제장 소셜네트워트를 꽃잔디 명소 곳 보 경남 산청 대명사 꽃 잔디 피 글 언급</td>\n",
       "      <td>경남 산청 대명사 꽃 잔디 만개 남도</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   postdate                                               body  \\\n",
       "0  20200108  안녕하세요 인생에 한번뿐인 여행이라고 불리기도 하는 신혼여행 그래서인지 허니문 여행...   \n",
       "1  20200107  당신을 위한 특별한 뉴스 티티엘입니다 종합여행기업 보물섬투어가 2020년 경자년을 ...   \n",
       "2  20240416  안녕하세요 의성군 블로그기자단 박진하입니다 오늘은 봄 향기가 가득한 단촌역 카페 이...   \n",
       "3  20200217  2020년 2월 3주차 여행지추천 삼성여행사 어플 다운로드 httpsgooglicA...   \n",
       "4  20240418  안녕하세요 짱가입니다 어제 산청꽃잔디축제를 살펴봤는데요 오늘은 생초조각공원과 더불어...   \n",
       "\n",
       "                                         description  \\\n",
       "0  그래서인지 허니문 b여행지b를 고를 때는 훨씬 더 신중하게 고르는데요 오늘은 b20...   \n",
       "1  특별한 경험 과거로 떠나는 미지의 여행 b2020년b에는 그동안 경험해 보지 못한 ...   \n",
       "2  2001년 보통역으로 재승격되었다가 b2020년b 9월 23일 중앙선 복선전철화 공...   \n",
       "3  b2020년b 2월 3주차 b여행지bb추천b 삼성여행사 어플 다운로드 httpsgo...   \n",
       "4  꽃잔디축제장이었다면 b2020년b 이후 소셜네트워트를 통해 아마 가장 유명한 꽃잔디...   \n",
       "\n",
       "                                     title  \\\n",
       "0         신혼 여행지 추천 2020년에 제일 핫할 신혼 여행지 5곳   \n",
       "1                 카드뉴스 2020년 경자년 핫플레이스 여행지   \n",
       "2  옛 추억의 소품들이 가득한 과거 여행지 추천 의성군 단촌역 카페 나들이   \n",
       "3                 삼성여행사 2020년 2월 3주차 여행지추천   \n",
       "4                  경남 산청대명사꽃잔디 만개 남도 4월여행지   \n",
       "\n",
       "                                   preprocessed_body  \\\n",
       "0  번 뿐 라 신혼 허니문 신중 것 은 핫 신혼 곳 것 발리 발리 신혼여행 동남아시아 ...   \n",
       "1  뉴스 티티엘 종합 기업 보물섬 경 보물 해외 제시 눈길 과거 지 그동안 수요 예측 ...   \n",
       "2  의성군 블로그 기자단 박진 은 봄 향기 단촌역 카페 단촌역 보통역 개통 되 오랫동안...   \n",
       "3  주 차 삼성 여행사 어플 다운로드 삼성 여행사 밴드 가입 하기 삼성 여행사 카카오톡...   \n",
       "4  산청 꽃잔디 축제 은 생초 조각 공원 꽃잔디 명소 산청 꽃잔디 시 터 데 산청 대명...   \n",
       "\n",
       "                            preprocessed_description       preprocessed_title  \\\n",
       "0               허니문 신중 것 은 핫 신혼 곳 것 발리 발리 신혼여행 동남아시아                  신혼 신혼 곳   \n",
       "1                       과거 지 그동안 수요 예측 보물섬 이색 문명 박물관        카드 뉴스 경 자녀 핫 플레이스   \n",
       "2  보통역 승격 중앙선 복선 전철 공사 무릉 촌 단초 고추 시장 시 단촌역 보 시 기 ...  추억 소품 과거 의성군 단촌역 카페 나들이   \n",
       "3   주 차 삼성 여행사 어플 다운로드 삼성 여행사 밴드 가입 하기 삼성 여행사 카카오톡 문               삼성 여행사 주 차   \n",
       "4   꽃잔디 축제장 소셜네트워트를 꽃잔디 명소 곳 보 경남 산청 대명사 꽃 잔디 피 글 언급     경남 산청 대명사 꽃 잔디 만개 남도   \n",
       "\n",
       "   계절  \n",
       "0   0  \n",
       "1   0  \n",
       "2   0  \n",
       "3   0  \n",
       "4   0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_travel.shape)\n",
    "df_travel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "봄의 개수: (543, 8)\n",
      "여름의 개수: (484, 8)\n",
      "가을의 개수: (365, 8)\n",
      "겨울의 개수: (529, 8)\n"
     ]
    }
   ],
   "source": [
    "sp = df_travel[df_travel['계절'] == 0]\n",
    "sm = df_travel[df_travel['계절'] == 1]\n",
    "at = df_travel[df_travel['계절'] == 2]\n",
    "wt = df_travel[df_travel['계절'] == 3]\n",
    "\n",
    "print(f'봄의 개수: {sp.shape}')\n",
    "print(f'여름의 개수: {sm.shape}')\n",
    "print(f'가을의 개수: {at.shape}')\n",
    "print(f'겨울의 개수: {wt.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여행지.txt 파일에 있는 단어들만 따로 뽑아서 컬럼 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여행 텍스트 파일에서 여행지 및 여행 관련 키워드 읽기\n",
    "def load_travel(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        travel_list = file.read().splitlines()\n",
    "    return travel_list\n",
    "\n",
    "# 파일에서 여행 목록 로드\n",
    "travel_file_path = '../../travel.txt'\n",
    "file_travel = load_travel(travel_file_path)\n",
    "\n",
    "# 여행 리스트 중복 삭제\n",
    "all_travel = list(set(file_travel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [동남아, 리조트, 발리, 신혼여행, 신혼, 자연, 휴양지, 동남아시아, 환경, 영...\n",
       "1       [박물관, 중국, 유럽, 트래킹, 자연, 절, 터키, 다낭, 바다, 호텔, 해변, ...\n",
       "2                                 [나들이, 전시, 카페, 벚꽃, 산, 꽃]\n",
       "3       [제주, 프로방스, 전망대, 매화, 광주, 거제, 벚꽃, 해남, 절, 인천, 목포,...\n",
       "4       [정원, 나들이, 개화, 나무, 잔디, 공원, 산, 사찰, 꽃잔디, 꽃, 남도, 경...\n",
       "                              ...                        \n",
       "1916              [세종, 나무, 산책, 서울, 국내, 대만, 산, 야경, 테마, 도심]\n",
       "1917    [유럽, 일본, 리조트, 근교, 전시, 국내, 광주, 워터파크, 데이트, 산, 케이...\n",
       "1918    [제주, 제주도, 관광지, 산책, 카페, 일출, 수국, 자연, 절, 공원, 산, 바...\n",
       "1919    [박물관, 유럽, 강원, 나무, 강원도, 전시, 국내, 하와이, 자연, 절, 드라이...\n",
       "1920    [강원, 나무, 썰매, 강원도, 대관령, 국내, 카페, 절, 바람, 평창, 산, 경...\n",
       "Name: keyword_lst, Length: 1921, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 키워드 컬럼 생성\n",
    "df_travel['keyword_lst'] = df_travel['preprocessed_body'].apply(lambda x: [word for word in all_travel if word in x])\n",
    "\n",
    "# 결과 출력\n",
    "df_travel['keyword_lst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       동남아 리조트 발리 신혼여행 신혼 자연 휴양지 동남아시아 환경 영국 산 스키 당일치...\n",
       "1            박물관 중국 유럽 트래킹 자연 절 터키 다낭 바다 호텔 해변 역사 베트남 나트랑\n",
       "2                                        나들이 전시 카페 벚꽃 산 꽃\n",
       "3       제주 프로방스 전망대 매화 광주 거제 벚꽃 해남 절 인천 목포 공원 다낭 산 바다 ...\n",
       "4                  정원 나들이 개화 나무 잔디 공원 산 사찰 꽃잔디 꽃 남도 경남 철쭉\n",
       "                              ...                        \n",
       "1916                         세종 나무 산책 서울 국내 대만 산 야경 테마 도심\n",
       "1917    유럽 일본 리조트 근교 전시 국내 광주 워터파크 데이트 산 케이블카 야경 테마 파크...\n",
       "1918       제주 제주도 관광지 산책 카페 일출 수국 자연 절 공원 산 바다 유채 폭포 동굴 꽃\n",
       "1919    박물관 유럽 강원 나무 강원도 전시 국내 하와이 자연 절 드라이브 터키 산 미국 스...\n",
       "1920         강원 나무 썰매 강원도 대관령 국내 카페 절 바람 평창 산 경상도 강릉 해변 꽃\n",
       "Name: keywords, Length: 1921, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_travel['keywords'] = df_travel['preprocessed_body'].apply(lambda x: ' '.join([word for word in all_travel if word in x]))\n",
    "\n",
    "df_travel['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postdate</th>\n",
       "      <th>body</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>preprocessed_body</th>\n",
       "      <th>preprocessed_description</th>\n",
       "      <th>preprocessed_title</th>\n",
       "      <th>계절</th>\n",
       "      <th>keyword_lst</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200108</td>\n",
       "      <td>안녕하세요 인생에 한번뿐인 여행이라고 불리기도 하는 신혼여행 그래서인지 허니문 여행...</td>\n",
       "      <td>그래서인지 허니문 b여행지b를 고를 때는 훨씬 더 신중하게 고르는데요 오늘은 b20...</td>\n",
       "      <td>신혼 여행지 추천 2020년에 제일 핫할 신혼 여행지 5곳</td>\n",
       "      <td>번 뿐 라 신혼 허니문 신중 것 은 핫 신혼 곳 것 발리 발리 신혼여행 동남아시아 ...</td>\n",
       "      <td>허니문 신중 것 은 핫 신혼 곳 것 발리 발리 신혼여행 동남아시아</td>\n",
       "      <td>신혼 신혼 곳</td>\n",
       "      <td>0</td>\n",
       "      <td>[동남아, 리조트, 발리, 신혼여행, 신혼, 자연, 휴양지, 동남아시아, 환경, 영...</td>\n",
       "      <td>동남아 리조트 발리 신혼여행 신혼 자연 휴양지 동남아시아 환경 영국 산 스키 당일치...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200107</td>\n",
       "      <td>당신을 위한 특별한 뉴스 티티엘입니다 종합여행기업 보물섬투어가 2020년 경자년을 ...</td>\n",
       "      <td>특별한 경험 과거로 떠나는 미지의 여행 b2020년b에는 그동안 경험해 보지 못한 ...</td>\n",
       "      <td>카드뉴스 2020년 경자년 핫플레이스 여행지</td>\n",
       "      <td>뉴스 티티엘 종합 기업 보물섬 경 보물 해외 제시 눈길 과거 지 그동안 수요 예측 ...</td>\n",
       "      <td>과거 지 그동안 수요 예측 보물섬 이색 문명 박물관</td>\n",
       "      <td>카드 뉴스 경 자녀 핫 플레이스</td>\n",
       "      <td>0</td>\n",
       "      <td>[박물관, 중국, 유럽, 트래킹, 자연, 절, 터키, 다낭, 바다, 호텔, 해변, ...</td>\n",
       "      <td>박물관 중국 유럽 트래킹 자연 절 터키 다낭 바다 호텔 해변 역사 베트남 나트랑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20240416</td>\n",
       "      <td>안녕하세요 의성군 블로그기자단 박진하입니다 오늘은 봄 향기가 가득한 단촌역 카페 이...</td>\n",
       "      <td>2001년 보통역으로 재승격되었다가 b2020년b 9월 23일 중앙선 복선전철화 공...</td>\n",
       "      <td>옛 추억의 소품들이 가득한 과거 여행지 추천 의성군 단촌역 카페 나들이</td>\n",
       "      <td>의성군 블로그 기자단 박진 은 봄 향기 단촌역 카페 단촌역 보통역 개통 되 오랫동안...</td>\n",
       "      <td>보통역 승격 중앙선 복선 전철 공사 무릉 촌 단초 고추 시장 시 단촌역 보 시 기 ...</td>\n",
       "      <td>추억 소품 과거 의성군 단촌역 카페 나들이</td>\n",
       "      <td>0</td>\n",
       "      <td>[나들이, 전시, 카페, 벚꽃, 산, 꽃]</td>\n",
       "      <td>나들이 전시 카페 벚꽃 산 꽃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200217</td>\n",
       "      <td>2020년 2월 3주차 여행지추천 삼성여행사 어플 다운로드 httpsgooglicA...</td>\n",
       "      <td>b2020년b 2월 3주차 b여행지bb추천b 삼성여행사 어플 다운로드 httpsgo...</td>\n",
       "      <td>삼성여행사 2020년 2월 3주차 여행지추천</td>\n",
       "      <td>주 차 삼성 여행사 어플 다운로드 삼성 여행사 밴드 가입 하기 삼성 여행사 카카오톡...</td>\n",
       "      <td>주 차 삼성 여행사 어플 다운로드 삼성 여행사 밴드 가입 하기 삼성 여행사 카카오톡 문</td>\n",
       "      <td>삼성 여행사 주 차</td>\n",
       "      <td>0</td>\n",
       "      <td>[제주, 프로방스, 전망대, 매화, 광주, 거제, 벚꽃, 해남, 절, 인천, 목포,...</td>\n",
       "      <td>제주 프로방스 전망대 매화 광주 거제 벚꽃 해남 절 인천 목포 공원 다낭 산 바다 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20240418</td>\n",
       "      <td>안녕하세요 짱가입니다 어제 산청꽃잔디축제를 살펴봤는데요 오늘은 생초조각공원과 더불어...</td>\n",
       "      <td>꽃잔디축제장이었다면 b2020년b 이후 소셜네트워트를 통해 아마 가장 유명한 꽃잔디...</td>\n",
       "      <td>경남 산청대명사꽃잔디 만개 남도 4월여행지</td>\n",
       "      <td>산청 꽃잔디 축제 은 생초 조각 공원 꽃잔디 명소 산청 꽃잔디 시 터 데 산청 대명...</td>\n",
       "      <td>꽃잔디 축제장 소셜네트워트를 꽃잔디 명소 곳 보 경남 산청 대명사 꽃 잔디 피 글 언급</td>\n",
       "      <td>경남 산청 대명사 꽃 잔디 만개 남도</td>\n",
       "      <td>0</td>\n",
       "      <td>[정원, 나들이, 개화, 나무, 잔디, 공원, 산, 사찰, 꽃잔디, 꽃, 남도, 경...</td>\n",
       "      <td>정원 나들이 개화 나무 잔디 공원 산 사찰 꽃잔디 꽃 남도 경남 철쭉</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   postdate                                               body  \\\n",
       "0  20200108  안녕하세요 인생에 한번뿐인 여행이라고 불리기도 하는 신혼여행 그래서인지 허니문 여행...   \n",
       "1  20200107  당신을 위한 특별한 뉴스 티티엘입니다 종합여행기업 보물섬투어가 2020년 경자년을 ...   \n",
       "2  20240416  안녕하세요 의성군 블로그기자단 박진하입니다 오늘은 봄 향기가 가득한 단촌역 카페 이...   \n",
       "3  20200217  2020년 2월 3주차 여행지추천 삼성여행사 어플 다운로드 httpsgooglicA...   \n",
       "4  20240418  안녕하세요 짱가입니다 어제 산청꽃잔디축제를 살펴봤는데요 오늘은 생초조각공원과 더불어...   \n",
       "\n",
       "                                         description  \\\n",
       "0  그래서인지 허니문 b여행지b를 고를 때는 훨씬 더 신중하게 고르는데요 오늘은 b20...   \n",
       "1  특별한 경험 과거로 떠나는 미지의 여행 b2020년b에는 그동안 경험해 보지 못한 ...   \n",
       "2  2001년 보통역으로 재승격되었다가 b2020년b 9월 23일 중앙선 복선전철화 공...   \n",
       "3  b2020년b 2월 3주차 b여행지bb추천b 삼성여행사 어플 다운로드 httpsgo...   \n",
       "4  꽃잔디축제장이었다면 b2020년b 이후 소셜네트워트를 통해 아마 가장 유명한 꽃잔디...   \n",
       "\n",
       "                                     title  \\\n",
       "0         신혼 여행지 추천 2020년에 제일 핫할 신혼 여행지 5곳   \n",
       "1                 카드뉴스 2020년 경자년 핫플레이스 여행지   \n",
       "2  옛 추억의 소품들이 가득한 과거 여행지 추천 의성군 단촌역 카페 나들이   \n",
       "3                 삼성여행사 2020년 2월 3주차 여행지추천   \n",
       "4                  경남 산청대명사꽃잔디 만개 남도 4월여행지   \n",
       "\n",
       "                                   preprocessed_body  \\\n",
       "0  번 뿐 라 신혼 허니문 신중 것 은 핫 신혼 곳 것 발리 발리 신혼여행 동남아시아 ...   \n",
       "1  뉴스 티티엘 종합 기업 보물섬 경 보물 해외 제시 눈길 과거 지 그동안 수요 예측 ...   \n",
       "2  의성군 블로그 기자단 박진 은 봄 향기 단촌역 카페 단촌역 보통역 개통 되 오랫동안...   \n",
       "3  주 차 삼성 여행사 어플 다운로드 삼성 여행사 밴드 가입 하기 삼성 여행사 카카오톡...   \n",
       "4  산청 꽃잔디 축제 은 생초 조각 공원 꽃잔디 명소 산청 꽃잔디 시 터 데 산청 대명...   \n",
       "\n",
       "                            preprocessed_description       preprocessed_title  \\\n",
       "0               허니문 신중 것 은 핫 신혼 곳 것 발리 발리 신혼여행 동남아시아                  신혼 신혼 곳   \n",
       "1                       과거 지 그동안 수요 예측 보물섬 이색 문명 박물관        카드 뉴스 경 자녀 핫 플레이스   \n",
       "2  보통역 승격 중앙선 복선 전철 공사 무릉 촌 단초 고추 시장 시 단촌역 보 시 기 ...  추억 소품 과거 의성군 단촌역 카페 나들이   \n",
       "3   주 차 삼성 여행사 어플 다운로드 삼성 여행사 밴드 가입 하기 삼성 여행사 카카오톡 문               삼성 여행사 주 차   \n",
       "4   꽃잔디 축제장 소셜네트워트를 꽃잔디 명소 곳 보 경남 산청 대명사 꽃 잔디 피 글 언급     경남 산청 대명사 꽃 잔디 만개 남도   \n",
       "\n",
       "   계절                                        keyword_lst  \\\n",
       "0   0  [동남아, 리조트, 발리, 신혼여행, 신혼, 자연, 휴양지, 동남아시아, 환경, 영...   \n",
       "1   0  [박물관, 중국, 유럽, 트래킹, 자연, 절, 터키, 다낭, 바다, 호텔, 해변, ...   \n",
       "2   0                            [나들이, 전시, 카페, 벚꽃, 산, 꽃]   \n",
       "3   0  [제주, 프로방스, 전망대, 매화, 광주, 거제, 벚꽃, 해남, 절, 인천, 목포,...   \n",
       "4   0  [정원, 나들이, 개화, 나무, 잔디, 공원, 산, 사찰, 꽃잔디, 꽃, 남도, 경...   \n",
       "\n",
       "                                            keywords  \n",
       "0  동남아 리조트 발리 신혼여행 신혼 자연 휴양지 동남아시아 환경 영국 산 스키 당일치...  \n",
       "1       박물관 중국 유럽 트래킹 자연 절 터키 다낭 바다 호텔 해변 역사 베트남 나트랑  \n",
       "2                                   나들이 전시 카페 벚꽃 산 꽃  \n",
       "3  제주 프로방스 전망대 매화 광주 거제 벚꽃 해남 절 인천 목포 공원 다낭 산 바다 ...  \n",
       "4             정원 나들이 개화 나무 잔디 공원 산 사찰 꽃잔디 꽃 남도 경남 철쭉  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_travel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 벡터 생성 -> keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_mat = tfidf.fit_transform(df_travel['keywords']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1921x196 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 27027 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_data, test_data로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = tfidf_mat.toarray()  ## 일반적인 넘파이 어레이로 바꿈 (상대적으로 작은 거라 바꾸는 게 도움이 될 것 같다고 하심)\n",
    "y = df_travel['계절'].to_numpy()  ## 있는 레이블 또한 넘파이로 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.20721411, 0.        ,\n",
       "        0.23316073],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.32334399, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn, x_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, stratify=y)  ## stratify=y: 레이블이 균등하게 분배가 되도록 설정하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.2549, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([2, 3, 3,  ..., 3, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "# 넘파이 배열을 텐서로 변환\n",
    "x_trn = torch.tensor(x_trn, dtype=torch.float32)\n",
    "y_trn = torch.tensor(y_trn, dtype=torch.long)\n",
    "x_tst = torch.tensor(x_tst, dtype=torch.float32)\n",
    "y_tst = torch.tensor(y_tst, dtype=torch.long)\n",
    "\n",
    "# 텐서 형태 출력\n",
    "print(x_trn)\n",
    "print(y_trn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 196])\n",
      "torch.Size([1536])\n",
      "torch.Size([385, 196])\n",
      "torch.Size([385])\n"
     ]
    }
   ],
   "source": [
    "print(x_trn.shape)\n",
    "print(y_trn.shape)\n",
    "print(x_tst.shape)\n",
    "print(y_tst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 만드는 class 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module): # nn.Module을 상속받아서 MyNet이라는 class를 생성\n",
    "    def __init__(self, in_dim, h1_dim, out_dim):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        # (1) 네트워크 구조 짜기 (with 단위 모듈)\n",
    "        self.fc1 = nn.Linear(in_dim, h1_dim) # input -> 1st hidden layer에 대한 FFNN\n",
    "        self.fc2 = nn.Linear(h1_dim, out_dim) # 1st -> output layer에 대한 FFNN\n",
    "\n",
    "    ## 어떻게 연결이 되는지 정의 -> x라는 인풋 데이터가 들어왔을 때 인풋에서 첫번째 레고를 거쳐가면서 중간에 어떤 값이 나오고... \n",
    "    ## 층을 3개 이상 쌓으면 과적합 증세 (?)가 나타나서 ...ㅜㅜ 층을 한 개만 쌓아봄\n",
    "    def forward(self, x):\n",
    "        # (2) Input Flow 짜기\n",
    "        h1 = self.fc1(x) \n",
    "        h1 = torch.sigmoid(h1) \n",
    "        h2 = self.fc2(h1) \n",
    "        out = F.log_softmax(h2, dim=1) \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클래스로부터 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## 실제 모델 객체를 하나 만들고 인풋 값을 그 객체에 흘려보내줄 것임\n",
    "\n",
    "# GPU사용 가능하면 GPU사용, 그렇지 않으면 CPU사용\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 이라는 변수에 실제 뉴럴 네트워크 사용\n",
    "\n",
    "model = MyNet(in_dim=196, # 입력하는 데이터의 차원 (196)\n",
    "            h1_dim=64, # 1st hidden layer의 neuron 수 (64) \n",
    "            out_dim=4) # data의 class 수 = 4 -> 계절\n",
    "model = model.to(device) # network를 'gpu' 혹은 'cpu'에 올려놓읍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (fc1): Linear(in_features=196, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader 생성\n",
    "### Mini-batch 데이터 생성기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알아서 매번 미니배치를 만들어주는 미니배치 생성기 -> 데이터 로더를 이용\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "train_dataset = TensorDataset(x_trn, y_trn)\n",
    "test_dataset = TensorDataset(x_tst, y_tst)\n",
    "\n",
    "trn_loader = DataLoader(dataset=train_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        drop_last=False)\n",
    "\n",
    "tst_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "my_opt = optim.Adam(params = model.parameters(), lr = 2e-3)\n",
    "\n",
    "## 훈련 데이터\n",
    "## lr = 2e-4일 때 ... 51.63%\n",
    "## lr = 2e-3일 때 ... 78.45%\n",
    "## lr = 2e-2일 때 ... 94.92% -> 과적합이 엄청남\n",
    "## lr = 1e-2일 때 ... 81.32%\n",
    "\n",
    "## 테스트 데이터\n",
    "## lr = 2e-4일 때 ... 48.052% \n",
    "## lr = 2e-3일 때 ... 65.974% -> loss가 가장 낮음\n",
    "## lr = 2e-2일 때 ... 66.494% \n",
    "## lr = 1e-2일 때 ... 62.597% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습\n",
    "## train() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, device):\n",
    "    model.train() # 학습모드\n",
    "    trn_loss = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = x.view(-1, 196).to(device) \n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = F.nll_loss(y_pred_prob, y, reduction='sum')\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Forward Propagation 수행\n",
    "### 하나의 Mini-batch data에 대해서 Forward Propagation 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 1.381 | Test Loss: 1.371 | Test Acc: 27.532% \n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 1.369 | Test Loss: 1.364 | Test Acc: 27.792% \n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 1.360 | Test Loss: 1.357 | Test Acc: 28.571% \n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 1.353 | Test Loss: 1.350 | Test Acc: 46.494% \n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 1.344 | Test Loss: 1.342 | Test Acc: 42.857% \n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 1.336 | Test Loss: 1.334 | Test Acc: 42.338% \n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 1.325 | Test Loss: 1.325 | Test Acc: 48.831% \n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 1.317 | Test Loss: 1.315 | Test Acc: 45.455% \n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 1.304 | Test Loss: 1.305 | Test Acc: 47.273% \n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 1.291 | Test Loss: 1.292 | Test Acc: 46.753% \n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 1.278 | Test Loss: 1.279 | Test Acc: 47.273% \n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 1.262 | Test Loss: 1.266 | Test Acc: 51.948% \n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 1.246 | Test Loss: 1.251 | Test Acc: 48.312% \n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 1.228 | Test Loss: 1.234 | Test Acc: 50.909% \n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 1.209 | Test Loss: 1.218 | Test Acc: 52.987% \n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 1.189 | Test Loss: 1.201 | Test Acc: 53.766% \n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 1.171 | Test Loss: 1.184 | Test Acc: 52.468% \n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 1.150 | Test Loss: 1.167 | Test Acc: 54.545% \n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 1.129 | Test Loss: 1.149 | Test Acc: 55.325% \n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 1.108 | Test Loss: 1.132 | Test Acc: 56.364% \n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 1.088 | Test Loss: 1.115 | Test Acc: 56.623% \n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 1.068 | Test Loss: 1.099 | Test Acc: 59.740% \n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 1.049 | Test Loss: 1.083 | Test Acc: 58.182% \n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 1.029 | Test Loss: 1.068 | Test Acc: 60.519% \n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 1.010 | Test Loss: 1.054 | Test Acc: 61.039% \n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.992 | Test Loss: 1.040 | Test Acc: 62.078% \n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.974 | Test Loss: 1.027 | Test Acc: 62.078% \n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.957 | Test Loss: 1.015 | Test Acc: 61.818% \n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.940 | Test Loss: 1.004 | Test Acc: 62.338% \n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.925 | Test Loss: 0.993 | Test Acc: 62.857% \n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.910 | Test Loss: 0.984 | Test Acc: 63.117% \n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.896 | Test Loss: 0.975 | Test Acc: 62.597% \n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.882 | Test Loss: 0.966 | Test Acc: 62.857% \n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.870 | Test Loss: 0.958 | Test Acc: 63.896% \n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.856 | Test Loss: 0.952 | Test Acc: 62.078% \n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.845 | Test Loss: 0.945 | Test Acc: 62.857% \n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.834 | Test Loss: 0.940 | Test Acc: 63.636% \n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.823 | Test Loss: 0.935 | Test Acc: 63.117% \n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.813 | Test Loss: 0.930 | Test Acc: 63.636% \n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.803 | Test Loss: 0.926 | Test Acc: 62.857% \n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.794 | Test Loss: 0.922 | Test Acc: 63.117% \n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.785 | Test Loss: 0.919 | Test Acc: 62.597% \n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.777 | Test Loss: 0.916 | Test Acc: 63.896% \n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.769 | Test Loss: 0.914 | Test Acc: 63.636% \n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.762 | Test Loss: 0.912 | Test Acc: 63.377% \n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.755 | Test Loss: 0.910 | Test Acc: 63.377% \n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.748 | Test Loss: 0.908 | Test Acc: 64.416% \n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.742 | Test Loss: 0.908 | Test Acc: 63.377% \n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.736 | Test Loss: 0.906 | Test Acc: 63.377% \n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.730 | Test Loss: 0.906 | Test Acc: 64.156% \n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터를 n_epoch번 반복하여 넣을 때 까지 학습합니다.\n",
    "n_epochs = 50\n",
    "\n",
    "# 매 epoch마다 반복\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    trn_loss = 0\n",
    "    # 매 mini-batch train data마다 반복\n",
    "    for i, (x, y) in enumerate(trn_loader):\n",
    "        # 1-(1): 모델에 입력하기 위해서 데이터의 형태 변환\n",
    "        x = x.view(-1,196).to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # 1-(2): 기존에 계산된 gradient를 0으로 reset\n",
    "        my_opt.zero_grad()\n",
    "        \n",
    "        # 1-(3): Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # 1-(4): Loss Calculation\n",
    "        loss = F.nll_loss(y_pred_prob, y, reduction = 'sum')\n",
    "        \n",
    "        # 1-(5): Gradient Calculation(Backprop)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 1-(6): Update parameter\n",
    "        my_opt.step()\n",
    "        \n",
    "        # 1-(7): trn_loss에 mini_batch loss를 누적해서 계산하기\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    trn_loss /= len(trn_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        # 매 mini-batch validation data마다 반복\n",
    "        for i, (x, y) in enumerate(tst_loader):\n",
    "            # 2-(1)\n",
    "            x = x.reshape(-1,196).to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # 2-(2)\n",
    "            y_pred_prob = model(x)\n",
    "            y_pred_label = torch.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "            # 2-(3)\n",
    "            loss = F.nll_loss(y_pred_prob, y, reduction='sum')\n",
    "            val_loss += loss\n",
    "            \n",
    "            results_pred.extend(y_pred_label.cpu().detach().numpy())\n",
    "            results_real.extend(y.cpu().detach().numpy())\n",
    "            \n",
    "        # 3.\n",
    "        val_loss /= len(tst_loader.dataset)\n",
    "        results_pred = np.array(results_pred)\n",
    "        results_real = np.array(results_real)\n",
    "        accuracy = np.sum(results_pred == results_real) / len(tst_loader.dataset)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가\n",
    "## evaluate() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, device):\n",
    "    model.eval() # 모델을 평가모드로!\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = x.view(-1,196).to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = F.nll_loss(y_pred_prob, y, reduction='sum')\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.724 | Test Loss: 0.905 | Test Acc: 63.377% \n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.720 | Test Loss: 0.904 | Test Acc: 63.636% \n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.714 | Test Loss: 0.904 | Test Acc: 64.935% \n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.709 | Test Loss: 0.904 | Test Acc: 64.675% \n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.704 | Test Loss: 0.904 | Test Acc: 64.675% \n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.701 | Test Loss: 0.904 | Test Acc: 63.896% \n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.696 | Test Loss: 0.905 | Test Acc: 64.416% \n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.692 | Test Loss: 0.905 | Test Acc: 64.416% \n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.688 | Test Loss: 0.906 | Test Acc: 64.675% \n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.685 | Test Loss: 0.906 | Test Acc: 64.416% \n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.680 | Test Loss: 0.907 | Test Acc: 64.416% \n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.677 | Test Loss: 0.908 | Test Acc: 64.675% \n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.674 | Test Loss: 0.908 | Test Acc: 64.416% \n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.671 | Test Loss: 0.909 | Test Acc: 64.416% \n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.668 | Test Loss: 0.910 | Test Acc: 64.416% \n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.664 | Test Loss: 0.911 | Test Acc: 64.675% \n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.663 | Test Loss: 0.913 | Test Acc: 63.896% \n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.660 | Test Loss: 0.914 | Test Acc: 64.935% \n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.657 | Test Loss: 0.914 | Test Acc: 64.675% \n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.655 | Test Loss: 0.916 | Test Acc: 64.935% \n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.652 | Test Loss: 0.917 | Test Acc: 64.675% \n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.650 | Test Loss: 0.917 | Test Acc: 64.675% \n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.647 | Test Loss: 0.919 | Test Acc: 64.935% \n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.644 | Test Loss: 0.921 | Test Acc: 64.675% \n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.642 | Test Loss: 0.922 | Test Acc: 64.935% \n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.640 | Test Loss: 0.923 | Test Acc: 64.156% \n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.639 | Test Loss: 0.925 | Test Acc: 64.156% \n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.636 | Test Loss: 0.925 | Test Acc: 65.195% \n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.635 | Test Loss: 0.927 | Test Acc: 64.935% \n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.633 | Test Loss: 0.929 | Test Acc: 63.896% \n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.630 | Test Loss: 0.931 | Test Acc: 64.416% \n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.629 | Test Loss: 0.933 | Test Acc: 64.156% \n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.627 | Test Loss: 0.933 | Test Acc: 64.935% \n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.625 | Test Loss: 0.934 | Test Acc: 64.935% \n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.624 | Test Loss: 0.935 | Test Acc: 64.156% \n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.622 | Test Loss: 0.938 | Test Acc: 64.156% \n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.621 | Test Loss: 0.938 | Test Acc: 64.935% \n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.620 | Test Loss: 0.940 | Test Acc: 64.416% \n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.618 | Test Loss: 0.943 | Test Acc: 64.156% \n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.617 | Test Loss: 0.942 | Test Acc: 64.675% \n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.616 | Test Loss: 0.944 | Test Acc: 64.416% \n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.614 | Test Loss: 0.947 | Test Acc: 63.896% \n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.613 | Test Loss: 0.947 | Test Acc: 64.416% \n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.611 | Test Loss: 0.949 | Test Acc: 64.675% \n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.610 | Test Loss: 0.950 | Test Acc: 64.675% \n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.609 | Test Loss: 0.952 | Test Acc: 64.416% \n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.608 | Test Loss: 0.953 | Test Acc: 64.675% \n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.606 | Test Loss: 0.955 | Test Acc: 64.935% \n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.606 | Test Loss: 0.956 | Test Acc: 64.935% \n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.604 | Test Loss: 0.958 | Test Acc: 64.675% \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model, \n",
    "                     data_loader=trn_loader, \n",
    "                     optimizer=my_opt, \n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=model, \n",
    "                                  data_loader=tst_loader, \n",
    "                                  optimizer=my_opt, \n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 모델의 정확도는 (in training set) 77.99%입니다.\n"
     ]
    }
   ],
   "source": [
    "trn_pred_results = []\n",
    "trn_real_results = []\n",
    "for i, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "    x_batch = x_batch.view(-1,196).to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred_score = model(x_batch)\n",
    "    y_pred_label = torch.argmax(y_pred_score, dim=1)\n",
    "    y_pred_label = y_pred_label.cpu().numpy()\n",
    "    y_real_label = y_batch.cpu().numpy()\n",
    "    trn_pred_results.extend(y_pred_label)\n",
    "    trn_real_results.extend(y_real_label)\n",
    "    \n",
    "trn_pred_results = np.array(trn_pred_results)\n",
    "trn_real_results = np.array(trn_real_results)\n",
    "accuracy = np.sum(trn_pred_results==trn_real_results) / len(trn_real_results)\n",
    "print(f'랜덤 모델의 정확도는 (in training set) {100*accuracy:.2f}%입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
