{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 유행어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kiwipiepy import Kiwi\n",
    "import copy\n",
    "import re\n",
    "from kiwipiepy.utils import Stopwords\n",
    "import nltk\n",
    "\n",
    "from wordcloud import WordCloud # 워드클라우드 제작 라이브러리\n",
    "import pandas as pd # 데이터 프레임 라이브러리\n",
    "import numpy as np # 행렬 라이브러리\n",
    "import matplotlib.pyplot as plt # 워드클라우드 시각화 라이브러리\n",
    "%matplotlib inline\n",
    "import konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postdate</th>\n",
       "      <th>body</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210106</td>\n",
       "      <td>제가 매년 연말마다 찾아보는 것 중에 하나예요. 일본에서는 매년 그해에 유행했거나,...</td>\n",
       "      <td>youtube.com\\/channel\\/UC1B51m7HSWGpm_qDDgoIeqA...</td>\n",
       "      <td>2020년 일본 유행어,신조어 대상 best 10.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210105</td>\n",
       "      <td>안녕! 아이비 친구들! 자, 오늘은 인싸력 레벨을 한 번 테스트해볼까? 2021년이...</td>\n",
       "      <td>2021년이 온 기념으로, 2020년 한 해를 빛낸 유행어는 어떤 것들이 있었는지 ...</td>\n",
       "      <td>[인싸력 테스트] 2020년을 빛낸 유행어 알아보기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20201218</td>\n",
       "      <td>안녕하세요, 멀리 보여도 늘 가까운 이웃나라, 일본. 핫하고 속 깊은 일본 소식을 ...</td>\n",
       "      <td>com\\/2020\\/04\\/2165\\/) 매년 12월이 되면 한 해 동안 유행했던 ...</td>\n",
       "      <td>2020년 유행어 대상(2020年 流行語大賞)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20201224</td>\n",
       "      <td>인터넷을 하다보면 처음 보는 줄임말이나 유행어들이 자주 보이는데요! 중국에는 어떤 ...</td>\n",
       "      <td>청년문적이 발표한 ‘&lt;b&gt;2020년&lt;\\/b&gt; 올해 인터넷에서 가장 반응이 뜨거웠던 ...</td>\n",
       "      <td>2020년 많이 쓰인 중국의 유행어 및 인터넷 용어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200601</td>\n",
       "      <td>으아 일하기시러 바로 시작 ㄹㅇㅋㅋ 혐오의 시대에 살고 있는 요즘, 한 가지 댓글이...</td>\n",
       "      <td>크크루삥뽕 트위치발 &lt;b&gt;유행어&lt;\\/b&gt;, 내지는 밈. 트위치는 도네이션, 아프리카...</td>\n",
       "      <td>2020년 유행어 트렌드 5월편 B급 주의</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   postdate                                               body  \\\n",
       "0  20210106  제가 매년 연말마다 찾아보는 것 중에 하나예요. 일본에서는 매년 그해에 유행했거나,...   \n",
       "1  20210105  안녕! 아이비 친구들! 자, 오늘은 인싸력 레벨을 한 번 테스트해볼까? 2021년이...   \n",
       "2  20201218  안녕하세요, 멀리 보여도 늘 가까운 이웃나라, 일본. 핫하고 속 깊은 일본 소식을 ...   \n",
       "3  20201224  인터넷을 하다보면 처음 보는 줄임말이나 유행어들이 자주 보이는데요! 중국에는 어떤 ...   \n",
       "4  20200601  으아 일하기시러 바로 시작 ㄹㅇㅋㅋ 혐오의 시대에 살고 있는 요즘, 한 가지 댓글이...   \n",
       "\n",
       "                                         description  \\\n",
       "0  youtube.com\\/channel\\/UC1B51m7HSWGpm_qDDgoIeqA...   \n",
       "1  2021년이 온 기념으로, 2020년 한 해를 빛낸 유행어는 어떤 것들이 있었는지 ...   \n",
       "2  com\\/2020\\/04\\/2165\\/) 매년 12월이 되면 한 해 동안 유행했던 ...   \n",
       "3  청년문적이 발표한 ‘<b>2020년<\\/b> 올해 인터넷에서 가장 반응이 뜨거웠던 ...   \n",
       "4  크크루삥뽕 트위치발 <b>유행어<\\/b>, 내지는 밈. 트위치는 도네이션, 아프리카...   \n",
       "\n",
       "                          title  \n",
       "0  2020년 일본 유행어,신조어 대상 best 10.  \n",
       "1  [인싸력 테스트] 2020년을 빛낸 유행어 알아보기  \n",
       "2     2020년 유행어 대상(2020年 流行語大賞)  \n",
       "3  2020년 많이 쓰인 중국의 유행어 및 인터넷 용어  \n",
       "4       2020년 유행어 트렌드 5월편 B급 주의  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('2020년 유행어.csv', encoding='utf-8')\n",
    "\n",
    "# postdate, body, description, title 칼럼만 사용 \n",
    "df = df[['postdate', 'body', 'description', 'title']]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 사용자 정의 단어 추가 \n",
    "kiwi = Kiwi()\n",
    "\n",
    "new = ['스라밸', '쌉파서블', '2000원', '머선', '머선129', '인성', '문제', '갑분싸', '국룰', \n",
    "       '삼귀다', '자만추', '레게노', '커엽', '존잘', '존귀', '졸귀', '존예', 'tmi', 'tmt', '띵작', \n",
    "       '팩폭', '레알', 'jmt', '갑툭튀', '먹방', '댕댕이', '존맛탱', 'ㄹㅇ', 'ㄹㅇㅋㅋ', '곁들인', '근데', '이제', \n",
    "       '꼬마아가씨', '꼬마', '아가씨']\n",
    "\n",
    "for i in new:\n",
    "    kiwi.add_user_word(i,'NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리할 칼럼들\n",
    "columns = ['body', 'description', 'title']\n",
    "\n",
    "# 중복 제거\n",
    "df.drop_duplicates(subset=columns, inplace=True)\n",
    "\n",
    "for column in columns:\n",
    "    # 정규표현식 - 한글, 숫자, 영어 및 공백 이외의 문자 제거\n",
    "    df[column] = df[column].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9a-zA-Z ]\", \"\", regex=True)\n",
    "    # 앞뒤 공백 제거\n",
    "    df[column] = df[column].str.strip()\n",
    "    # 여러 개의 공백을 하나의 공백으로 변경\n",
    "    df[column] = df[column].str.replace(' +', \" \", regex=True)\n",
    "\n",
    "# 공백만 있는 칼럼을 NaN으로 변경 후 제거\n",
    "df[columns] = df[columns].replace('', np.nan)\n",
    "df.dropna(subset=columns, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# Kiwi 초기화\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 불용어 리스트 정의 및 파일 로드\n",
    "additional_stopwords = ['유행어', '신조어', '2020년', '생각', '올해', '시간', '사람', '코로나', '대하', '시작', \n",
    "                        '일본', '사용', '한국', '사랑', '때문', '중국', '사진', '미국', '사회', '시장' \n",
    "                        '의미', '영상', '단어', '표현', '블로그', '정도', '인기', '배우', '모습', '이야기', \n",
    "                        '활동', '세대', '부모', '출처', '세계', '마음', '친구', '가지', '유행', '행복', \n",
    "                        '작품', '출연',  '이후', '차트', '상황', '다양', '관련', '최고', '멤버', '제공',\n",
    "                        '가능', '게임', '대상', '처음', '일상', '요즘', '보이', '여행', '진행', '프로그램', \n",
    "                        '시대', '앨범', '소개', '성공', '느낌', '정보', '콘텐츠','네이버', '오늘', '발매', \n",
    "                        '생활', '내용', '기업', '투자', '음악', '경제', '대표', '캐릭터', '시장', '광고', \n",
    "                        '트렌드', '이름', '부분', '엄마', '필요', '가족', '기억', '이유', '인터넷', '경우', \n",
    "                        '성장', '세상', '관심', '당시', '공부', '온라인', '감사', '마지막', '대화', '시즌', \n",
    "                        '등장', '기록', '사이', '준비', '콘서트', '인하', '영어', '공연', '데뷔', '발표', \n",
    "                        '공개', '개인', '나라', '중요', '자리', '회사', '여자', '서울', '도전', '교육', \n",
    "                        '기준', '채널', '운동', '순위', '대한민국', '변화', '소비', '제품', '경험', '가격', \n",
    "                        '마스크', '판매', '감독', '추천', '기술', '포스팅', '국내', '학교', '여성', '하루', \n",
    "                        '스타', '소통', '영국', '나이', '유명', '이미지', '청년', '건강', '카페', '방법', \n",
    "                        '이웃', '연기', '활용', '지금', '어머니', '정리', '운영', '작년', '소리', '남자', \n",
    "                        '인생', '서비스', '제작', '정부', '중국어', '시절', '노력', '기사', '하나님', '개그맨', \n",
    "                        '예수', '가수', '그룹', '화제',  '브랜드', '실패', '문장', '일본어', '이해', '거리', \n",
    "                        '언니', '진입', '최근', '결과', '물건','취미', '미래', '이용', '코미디', '아버지', \n",
    "                        '기대', '충격', '선택', '현실', '평가', '얼굴', '과거', '사건', '아이돌', '설명', \n",
    "                        '아래', '수업', '지역', '참여', '과정', '확인', '공간', '고민', '인간', '리뷰', \n",
    "                        '지원', '아빠', '아들', '예능', '시청', '커피', '개봉', '목표', '출시', '사실', \n",
    "                        '기분', '네이버', '의미', '대학', '환경', '부동산', '국민', '자체', '상태', '인물',\n",
    "                        '방식', '해당', '플랫폼', '말씀', '장면','음식', '능력', '주식', '싱글', '자녀', \n",
    "                        '블로그', '기간', '선정', '릴레이', '산업', '본인', '수익', '주인공', '마케팅', '증가', \n",
    "                        '분야', '선물', '결혼', '국가', '순간','수상', '주목', '유튜버',  '구매', '존재', \n",
    "                        '검색', '영향', '수준', '행동', '대중', '디지털', '년대', '관계', '자연', '계획', \n",
    "                        '뉴스', '공감', '안녕하세요', '아침', '머리','용어', '작가', '선생', '도움', '소식', \n",
    "                        '질문', '라면', '가치', '관리', '결정', '코너', '분위기', '효과', '한류', '학생',\n",
    "                        '언어', '동영상', '역사', '지목', '해외', '현상', '탄생', '현재' '예전', '주제', \n",
    "                        '감염', '무대', '중심', '데이터', '역할', '기회', '상승', '시기', '생산', '예정', \n",
    "                        '유지', '목소리', '개발', '제목', '기존', '공식', '단계', '사업', '소설', '언급', '얘기',\n",
    "                        '스토리', '남편', '분석', '정책', '프로', '대신', '저녁', '부문', '시리즈', '주변', \n",
    "                        '인사', '코미디언', '소비자', '어워드', '선수','개월', '도시', '대사', '상품', '맥주', \n",
    "                        '방영', '리그', '모델', '기본', '전체', '스타일', '졸업', '인상', '인증', '바이러스',\n",
    "                        '특징', '집중', '포함', '차이', '일반', '발생', '지식','발전', '발견', '직장', \n",
    "                        '한국어', '주류', '다이어리', '추억', '마무리', '글로벌', '오빠', '촬영', '반응', '예상', \n",
    "                        '경쟁', '전략', '걱정', '전문', '디자인','덕분', '구성', '지속', '센터', '이전', \n",
    "                        '조사', '이벤트', '공유', '행사', '출생', '정치', '시험', '시청자', '공경', '감정', \n",
    "                        '오랜만', '추가', '연출', '예술', '프로젝트', '규모', '형태', '정신', '논란', '사례',\n",
    "                        '맛집', '본문', '부담', '상대', '이제', '확대','하늘', '확산', '일부', '부족', '인터뷰', \n",
    "                        '검사', '약속', '취업', '작성', '장르', '연결', '장소', '신청', '연예', '성격', \n",
    "                        '경기', '시스템', '프로필', '연예인', '여름', '흥행', '바람', '아파트', '비교', '달러', \n",
    "                        '고객', '가운데', '대로', '동생', '연말', '특별', '입장', '내년', '감동', '예측', \n",
    "                        '위치', '반영', '위기', '기획',  '가정', '새해', '동시', '생일', '번역', '패션', \n",
    "                        '판매량', '청소년', '위스키', '방문', '캠핑'\n",
    "                        ]\n",
    "\n",
    "# 불용어 텍스트 파일에서 불용어 읽기\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords_list = file.read().splitlines()\n",
    "    return stopwords_list\n",
    "\n",
    "# 파일에서 불용어 목록 로드\n",
    "stopwords_file_path = '../stopwords.txt'\n",
    "file_stopwords = load_stopwords(stopwords_file_path)\n",
    "\n",
    "# 불용어 리스트 결합\n",
    "all_stopwords = set(additional_stopwords + file_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수 정의\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    filtered_tokens = [token.form for token in tokens if token.form not in stopwords]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_korean(text, analyzer=kiwi, stopwords=all_stopwords):\n",
    "    my_text = copy.copy(text)\n",
    "    my_text = my_text.replace('\\n', ' ') # (1) 줄바꿈 문자 제거\n",
    "    my_text = analyzer.space(my_text) # (2) 띄어쓰기 교정\n",
    "    sents = analyzer.split_into_sents(my_text) # (3) 문장 토큰화\n",
    "    p = re.compile('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]')\n",
    "    all_result = []\n",
    "    for sent in sents:\n",
    "        token_result = remove_stopwords(sent.text, stopwords) # (4) 형태소 분석 및 불용어 제거\n",
    "        token_result = p.sub(' ', token_result) # (5) 특수 문자 제거 (=한글을 제외한 문자 제거)\n",
    "        all_result.append(token_result) # (6) 형태소 분석한 결과를 다시 join\n",
    "    \n",
    "    all_result = ' '.join(all_result) # (7) 모든 문장을 하나의 string으로 join\n",
    "\n",
    "    return all_result\n",
    "\n",
    "# 품사(명사, 동사, 형용사, 부사) 추출 함수 정의\n",
    "def wordclass_korean(my_str, kiwi=kiwi):\n",
    "    result = []\n",
    "    tokens = kiwi.tokenize(my_str, normalize_coda=True)\n",
    "    for token in tokens:\n",
    "        if token.tag in ['NNG', 'NNP', 'NNB']:  # 명사 태그만 추출\n",
    "            result.append(token.form)\n",
    "    result = ' '.join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 전처리 함수 적용\n",
    "for column in columns:\n",
    "    preprocessed_column = 'preprocessed_' + column \n",
    "    df[preprocessed_column] = df[column].apply(lambda x: preprocess_korean(x))\n",
    "\n",
    "# 품사 추출 함수 적용\n",
    "preprocessed_columns = ['preprocessed_body', 'preprocessed_title', 'preprocessed_description']\n",
    "for column in preprocessed_columns:\n",
    "    # wordclass_column = 'wordclass_' + column \n",
    "    df[column] = df[column].apply(wordclass_korean)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리한 결과 csv로 저장 \n",
    "df.to_csv('20유행어_전처리.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 단어 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도 계산 및 상위 단어 추출 \n",
    "def explode_and_count(df, column):\n",
    "    exploded = df[column].str.split().explode()  # 문자열을 단어 단위로 나누어 행으로 펼침\n",
    "    word_counts = exploded.value_counts().reset_index()  # 단어 빈도 계산\n",
    "    word_counts.columns = ['word', 'count']  # 컬럼명 설정\n",
    "    return word_counts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [preprocessed_body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 칼럼에 대해 단어 빈도 계산\n",
    "word_counts_list = []\n",
    "for column in ['preprocessed_body']:\n",
    "    word_counts = explode_and_count(df, column)\n",
    "    word_counts_list.append(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 칼럼의 단어 빈도 합산 및 정렬\n",
    "all_words_body = pd.concat(word_counts_list).groupby('word').sum().reset_index().sort_values(by='count', ascending=False)\n",
    "\n",
    "# 한 글자 단어 제외\n",
    "all_words_body = all_words_body[all_words_body['word'].str.len() > 1]\n",
    "\n",
    "# 상위 30개 단어 출력\n",
    "top_30_words = all_words_body.head(200)\n",
    "top_30_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in top_30_words['word']:\n",
    "    l.append(i)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 한 번 해본 것\n",
    "\n",
    "for i in all_words_body['word']:\n",
    "    if i in new:\n",
    "        count = all_words_body.loc[all_words_body['word'] == i, 'count'].values[0]\n",
    "        print(f\"The count for '{i}' is: {count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [preprocessed_description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도 계산 및 상위 단어 추출 \n",
    "def explode_and_count(df, column):\n",
    "    exploded = df[column].str.split().explode()  # 문자열을 단어 단위로 나누어 행으로 펼침\n",
    "    word_counts = exploded.value_counts().reset_index()  # 단어 빈도 계산\n",
    "    word_counts.columns = ['word', 'count']  # 컬럼명 설정\n",
    "    return word_counts\n",
    "\n",
    "# 각 칼럼에 대해 단어 빈도 계산\n",
    "word_counts_list = []\n",
    "for column in ['preprocessed_title']:\n",
    "    word_counts = explode_and_count(df, column)\n",
    "    word_counts_list.append(word_counts)\n",
    "\n",
    "# 모든 칼럼의 단어 빈도 합산 및 정렬\n",
    "all_word_counts = pd.concat(word_counts_list).groupby('word').sum().reset_index().sort_values(by='count', ascending=False)\n",
    "\n",
    "# 한 글자 단어 제외\n",
    "all_word_counts = all_word_counts[all_word_counts['word'].str.len() > 1]\n",
    "\n",
    "# 상위 30개 단어 출력\n",
    "top_30_words = all_word_counts.head(30)\n",
    "top_30_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [preprocessed_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도 계산 및 상위 단어 추출 \n",
    "def explode_and_count(df, column):\n",
    "    exploded = df[column].str.split().explode()  # 문자열을 단어 단위로 나누어 행으로 펼침\n",
    "    word_counts = exploded.value_counts().reset_index()  # 단어 빈도 계산\n",
    "    word_counts.columns = ['word', 'count']  # 컬럼명 설정\n",
    "    return word_counts\n",
    "\n",
    "# 각 칼럼에 대해 단어 빈도 계산\n",
    "word_counts_list = []\n",
    "for column in ['preprocessed_description']:\n",
    "    word_counts = explode_and_count(df, column)\n",
    "    word_counts_list.append(word_counts)\n",
    "\n",
    "# 모든 칼럼의 단어 빈도 합산 및 정렬\n",
    "all_word_counts = pd.concat(word_counts_list).groupby('word').sum().reset_index().sort_values(by='count', ascending=False)\n",
    "\n",
    "# 한 글자 단어 제외\n",
    "all_word_counts = all_word_counts[all_word_counts['word'].str.len() > 1]\n",
    "\n",
    "# 상위 30개 단어 출력\n",
    "top_30_words = all_word_counts.head(30)\n",
    "top_30_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 참고 사이트\n",
    "# ## -> https://foreverhappiness.tistory.com/30\n",
    "# ## -> https://foreverhappiness.tistory.com/35\n",
    "# ## -> https://foreverhappiness.tistory.com/37\n",
    "# ## 너무 오래 걸려서 안 돌림\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from konlpy.tag import Okt\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # DTM (Document-Term Matrix) creation using Scikit-Learn's CountVectorizer\n",
    "# # DTM(Document-Term Matrix) 생성 함수\n",
    "# def NLP_DTM(df):\n",
    "#     # 타이틀 리스트를 데이터프레임의 'preprocessed_title' 열에서 가져옴\n",
    "#     title_lst = df['preprocessed_title']\n",
    "    \n",
    "#     # 불용어 리스트를 정의\n",
    "#     stop_words_list = ['사람', '대하']\n",
    "    \n",
    "#     # Okt 형태소 분석기를 초기화\n",
    "#     tagger = Okt()\n",
    "\n",
    "#     # 결과를 저장할 DataFrame 초기화\n",
    "#     result_df = pd.DataFrame()\n",
    "\n",
    "#     # 타이틀 리스트를 순회하며 진행도를 표시\n",
    "#     for title in tqdm(title_lst, desc='타이틀 리스트 진행도'):\n",
    "#         # CountVectorizer 객체를 초기화\n",
    "#         cv = CountVectorizer()\n",
    "\n",
    "#         # 각 문서의 말뭉치를 저장할 리스트를 초기화\n",
    "#         corpus = []\n",
    "\n",
    "#         # 문서 진행도를 표시하며 타이틀 리스트의 각 요소에 대해 반복\n",
    "#         for i in tqdm(range(len(df['preprocessed_body'])), desc='문서 진행도'):\n",
    "#             # 각 타이틀에 대해 명사 리스트를 생성합니다.\n",
    "#             n_lst = tagger.nouns(df['preprocessed_body'].iloc[i])\n",
    "#             corpus.append(' '.join(n_lst))\n",
    "\n",
    "#         # 말뭉치 데이터를 사용해 DTM(Document-Term Matrix)을 생성\n",
    "#         DTM_array = cv.fit_transform(corpus).toarray()\n",
    "        \n",
    "#         # DTM의 각 열이 어떤 단어에 해당하는지 feature_names에 저장\n",
    "#         feature_names = cv.get_feature_names_out()\n",
    "\n",
    "#         # DTM 배열을 DataFrame 형식으로 변환\n",
    "#         DTM_df = pd.DataFrame(DTM_array, columns=feature_names)\n",
    "        \n",
    "#         # 불용어 리스트에 있는 단어들을 DTM DataFrame에서 제거\n",
    "#         DTM_df.drop(columns=stop_words_list, inplace=True, errors='ignore')\n",
    "        \n",
    "#         # 결과 DataFrame에 현재 DTM DataFrame을 추가\n",
    "#         result_df = pd.concat([result_df, DTM_df], axis=0)\n",
    "\n",
    "#     return result_df\n",
    "\n",
    "\n",
    "# # %%time\n",
    "# dtm_df = NLP_DTM(df)\n",
    "# dtm_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교수님 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DTM (Document Term Matrix)\n",
    "## 이거 안 하고 바로 TF-IDF로 가도 됨\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "documents = df['preprocessed_body']\n",
    "word_dict = all_word_counts['word']\n",
    "vector = CountVectorizer(vocabulary=word_dict)\n",
    "results = vector.fit_transform(documents).toarray()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = df['preprocessed_body']\n",
    "word_dict = all_word_counts['word']\n",
    "vector = CountVectorizer(vocabulary=word_dict)\n",
    "dtm = vector.fit_transform(documents).toarray()\n",
    "\n",
    "\n",
    "vector = TfidfVectorizer(vocabulary=word_dict)\n",
    "tfidf = vector.fit_transform(documents).toarray()\n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 가정: df는 미리 전처리된 텍스트 데이터 프레임\n",
    "documents = df['preprocessed_body']\n",
    "word_dict = all_word_counts['word'].tolist()\n",
    "\n",
    "# TF-IDF 계산\n",
    "vectorizer = TfidfVectorizer(vocabulary=word_dict)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# 각 단어의 TF-IDF 값을 집계\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 데이터 프레임 생성\n",
    "tfidf_df = pd.DataFrame({'words': words, 'counts': tfidf_scores})\n",
    "\n",
    "# TF-IDF 값이 큰 순서로 정렬\n",
    "tfidf_df = tfidf_df.sort_values(by='counts', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 한 번 해본 것\n",
    "\n",
    "for i in tfidf_df['words']:\n",
    "    if i in new:\n",
    "        count = tfidf_df.loc[tfidf_df['words'] == i, 'counts'].values[0]\n",
    "        print(f\"The count for '{i}' is: {count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 워드 클라우드"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud # 워드클라우드 제작 라이브러리\n",
    "import pandas as pd # 데이터 프레임 라이브러리\n",
    "import numpy as np # 행렬 라이브러리\n",
    "import matplotlib.pyplot as plt # 워드클라우드 시각화 라이브러리\n",
    "%matplotlib inline\n",
    "\n",
    "import konlpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud 클래스의 객체 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 사이트: https://serendipity77.tistory.com/entry/영화동감-영화동감-댓글-리뷰-워드-클라우드wordcloud로-만들어보기파이썬-Python \n",
    "\n",
    "[엑셀 자동화로 칼퇴하는 김대리의 고군분투기:티스토리]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path='C:\\\\Windows\\\\Fonts\\\\malgun.ttf'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [body WordCloud]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 프레임을 딕셔너리 형태로 변환해야 함\n",
    "\n",
    "dic_word = all_words_body.set_index('word').to_dict()['count']\n",
    "dic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## colormap 참고 사이트: https://wonhwa.tistory.com/20\n",
    "\n",
    "wc = WordCloud(random_state = 123, font_path = font_path, width = 400,\n",
    "               height = 400, background_color = 'black',\n",
    "               colormap = 'Purples')\n",
    "               ## 'Blues': 파란색 그라데이션\n",
    "               ## 'BuGn': 청록색\n",
    "               ## 'BuPu': 진보라 ~ 연한 파란색 ~ 하얀색 \n",
    "               ## 'GnBu': 진파랑 ~ 연한 초록색 ~ 하얀색\n",
    "               ## 'Greys': 검은색 ~ 회색 ~ 하얀색\n",
    "               ## 'OrRd': 진빨강 ~ 연한 오렌지 ~ 하얀색\n",
    "               ## 'Pastel1': 파스텔 색\n",
    "               ## 'Pastel2': 파스텔 색\n",
    "               ## 'PuBu': 진파랑 ~ 핑크 ~ 하얀색\n",
    "               ## 'PuRd': 자주 ~ 핑크 ~ 하얀색\n",
    "               ## 'Purples': 보라색\n",
    "               ## 'RdPu': 보라 ~ 자주 ~ 핑크\n",
    "               ## 'Reds': 빨강\n",
    "               ## 'Wistia': 주황 ~ 형광 노랑\n",
    "               ## 'YlGnBr': 파랑 ~ 초록 ~ 노랑\n",
    "\n",
    "\n",
    "img_wordcloud = wc.generate_from_frequencies(dic_word)\n",
    "\n",
    "plt.figure(figsize = (10, 10)) # 크기 지정하기\n",
    "plt.axis('off') # 축 없애기\n",
    "plt.imshow(img_wordcloud) # 결과 보여주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
